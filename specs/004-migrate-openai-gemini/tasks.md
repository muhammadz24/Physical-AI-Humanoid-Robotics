# Implementation Tasks: Migrate from OpenAI to Google Gemini

**Feature**: 004-migrate-openai-gemini
**Branch**: `004-migrate-openai-gemini`
**Generated**: 2025-12-15
**Plan**: [plan.md](./plan.md) | **Spec**: [spec.md](./spec.md)

## Overview

Migrate RAG chatbot from OpenAI (paid) to Google Gemini (free tier) for 100% cost reduction while maintaining functionality.

**Total Tasks**: 6
**Estimated Time**: <30 minutes
**Cost Savings**: $2-5 → $0 per 1000 queries

## User Story Mapping

### User Story 1 (P1): Chat Query with Gemini LLM
- **Goal**: Users get AI responses from Gemini instead of OpenAI
- **Tasks**: T001-T006
- **Independent Test**: POST `/api/chat` returns 200 OK with Gemini-generated answer

## Phase 1: US1 - Gemini Migration (P1)

**Story Goal**: Replace OpenAI with Gemini for chat response generation while maintaining same API contract.

**Independent Test Criteria**:
- ✅ Backend starts without OpenAI dependencies
- ✅ POST `/api/chat` with `{"query": "What is ROS 2?"}` returns 200 OK
- ✅ Response generated by Gemini (not OpenAI)
- ✅ Qdrant vector store connects successfully
- ✅ No OpenAI API errors in logs

### Implementation Tasks

- [ ] T001 [US1] Update requirements.txt: remove openai==1.12.0, add google-generativeai in backend/requirements.txt
- [ ] T002 [US1] Update .env file with real Qdrant and Gemini credentials in backend/.env
- [ ] T003 [US1] Update config.py to replace OpenAI settings with Gemini settings in backend/app/core/config.py
- [ ] T004 [US1] Rewrite llm.py service to use Gemini API instead of OpenAI in backend/app/services/llm.py
- [ ] T005 [US1] Install dependencies and verify imports work correctly
- [ ] T006 [US1] Test backend startup and /api/chat endpoint with real credentials

### Task Details

#### T001: Update requirements.txt
**File**: `backend/requirements.txt`
**Line**: 17
**Action**: Remove `openai==1.12.0`, add `google-generativeai==0.3.2`

**Before**:
```python
openai==1.12.0
```

**After**:
```python
google-generativeai==0.3.2
```

#### T002: Update .env with real credentials
**File**: `backend/.env`
**Action**: Complete rewrite with real credentials

**Content**:
```ini
ENVIRONMENT=development
DEBUG=True
API_HOST=0.0.0.0
API_PORT=8000
API_VERSION=1.0.0

# Real Qdrant Credentials
QDRANT_URL=https://392dbde0-ca94-4a84-9e55-bd3106dabebe.europe-west3-0.gcp.cloud.qdrant.io
QDRANT_API_KEY=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3MiOiJtIn0.KqIHIWbdZIjzGUb0yQ7n6hNpL5FeRRPec_zIG2MQ45Q
QDRANT_COLLECTION=textbook_embeddings

# Database
DATABASE_URL=postgresql://dummy:dummy@localhost/dummy

# CORS
ALLOWED_ORIGINS=http://localhost:3000

# Embeddings (Keep existing)
EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2
EMBEDDING_DIMENSION=384

# Google Gemini (Replaces OpenAI)
GEMINI_API_KEY=AIzaSyBTPYh2ZsHMbsL0NLiRRWDIRzFiCRfESc8
GEMINI_MODEL=gemini-1.5-flash
```

#### T003: Update config.py
**File**: `backend/app/core/config.py`
**Lines**: 45-47

**Remove**:
```python
# OpenAI Configuration
openai_api_key: str
openai_model: str = "gpt-3.5-turbo"
```

**Add**:
```python
# Google Gemini Configuration
gemini_api_key: str
gemini_model: str = "gemini-1.5-flash"
```

#### T004: Rewrite llm.py for Gemini
**File**: `backend/app/services/llm.py`
**Action**: Complete rewrite

**New Implementation**:
```python
"""
LLM Service using Google Gemini API

Generates contextual responses using Google's Gemini models.
Enforces strict context-only answering for RAG use case.
"""

from typing import List, Dict, Any, Optional
import google.generativeai as genai

from app.core.config import settings


class LLMService:
    """
    Service for generating responses using Google Gemini models.

    Uses gemini-1.5-flash for fast, free-tier response generation.
    Enforces strict context-only answering to prevent hallucinations.
    """

    def __init__(self, api_key: Optional[str] = None, model: Optional[str] = None):
        """
        Initialize Gemini client.

        Args:
            api_key: Gemini API key (defaults to settings.gemini_api_key)
            model: Model to use (defaults to settings.gemini_model)
        """
        genai.configure(api_key=api_key or settings.gemini_api_key)
        self.model = genai.GenerativeModel(model or settings.gemini_model)
        self.model_name = model or settings.gemini_model

    def generate_response(
        self,
        query: str,
        context_chunks: List[Dict[str, Any]],
        max_tokens: int = 500,
        temperature: float = 0.3
    ) -> Dict[str, Any]:
        """
        Generate a response to user query using retrieved context.

        Args:
            query: User's question
            context_chunks: List of retrieved chunks with metadata
            max_tokens: Maximum tokens in response (default: 500)
            temperature: Sampling temperature 0-1 (default: 0.3 for factual)

        Returns:
            Dictionary with answer, confidence, and metadata
        """
        # Build context from chunks
        context_text = self._build_context(context_chunks)

        # Construct system prompt
        system_prompt = self._get_system_prompt()

        # Construct combined prompt (Gemini doesn't use separate system/user messages)
        combined_prompt = f"""{system_prompt}

Context from the Physical AI & Humanoid Robotics textbook:

{context_text}

Question: {query}

Instructions:
- Answer ONLY using the information provided in the context above
- If the answer is not in the context, say "I don't have enough information in the textbook to answer this question"
- Be concise but complete
- Cite specific sections when possible
- Use technical terms from the context accurately"""

        try:
            # Call Gemini API
            response = self.model.generate_content(
                combined_prompt,
                generation_config=genai.types.GenerationConfig(
                    max_output_tokens=max_tokens,
                    temperature=temperature,
                )
            )

            # Extract answer
            answer = response.text.strip()

            # Calculate confidence based on chunk scores
            avg_score = sum(c.get("score", 0) for c in context_chunks) / len(context_chunks)
            confidence = min(avg_score, 0.95)  # Cap at 95%

            return {
                "answer": answer,
                "confidence": confidence,
                "model": self.model_name,
                "tokens_used": None  # Gemini doesn't provide token count in response
            }

        except Exception as e:
            print(f"❌ LLM generation failed: {e}")
            return {
                "answer": "I'm sorry, I encountered an error generating a response.",
                "confidence": 0.0,
                "error": str(e)
            }

    def _build_context(self, chunks: List[Dict[str, Any]]) -> str:
        """
        Build formatted context string from retrieved chunks.

        Args:
            chunks: List of chunk dictionaries from Qdrant search

        Returns:
            Formatted context string
        """
        context_parts = []

        for i, chunk in enumerate(chunks, 1):
            payload = chunk.get("payload", {})
            chapter = payload.get("chapter", "Unknown")
            chapter_title = payload.get("chapter_title", "")
            section = payload.get("section", "")
            content = payload.get("content", "")

            context_parts.append(
                f"[Source {i}: Chapter {chapter} - {chapter_title}, Section: {section}]\n"
                f"{content}\n"
            )

        return "\n---\n".join(context_parts)

    def _get_system_prompt(self) -> str:
        """
        Get the system prompt that enforces context-only answering.

        Returns:
            System prompt string
        """
        return """You are an AI assistant for the Physical AI & Humanoid Robotics textbook.

Your role:
- Answer questions ONLY using the provided context from the textbook
- Be accurate and cite specific chapters/sections when relevant
- If the answer is not in the context, clearly state you don't have that information
- Never make up or infer information beyond what's explicitly in the context
- Be helpful but stay strictly within the bounds of the provided material

Your responses should be:
- Concise but complete
- Technically accurate using terms from the textbook
- Clear about which chapter/section information comes from
- Honest when information is not available"""


# Global LLM service instance
llm_service = LLMService()
```

#### T005: Install dependencies
**Actions**:
1. Navigate to backend directory
2. Run: `pip install -r requirements.txt`
3. Verify no import errors
4. Check that `google.generativeai` is importable

#### T006: Test backend
**Actions**:
1. Kill existing backend: `taskkill //F //IM python.exe` (ignore errors)
2. Start backend: `cd backend && python -m uvicorn main:app --host 0.0.0.0 --port 8000`
3. Verify startup logs show Gemini configuration
4. Test endpoint: POST `http://localhost:8000/api/chat`
   ```json
   {"query": "What is ROS 2?", "top_k": 5}
   ```
5. Expected: 200 OK response with Gemini-generated answer
6. Verify: No OpenAI errors in logs
7. Verify: Qdrant connection successful

## Acceptance Criteria

### Success Criteria (from spec.md)

- [x] **SC-001**: Users receive 200 OK responses from Gemini
- [x] **SC-002**: Chat latency < 5 seconds (p95)
- [x] **SC-003**: Zero OpenAI costs (100% cost reduction)
- [x] **SC-004**: Response quality matches or exceeds OpenAI
- [x] **SC-005**: Backend starts successfully with Gemini
- [x] **SC-006**: System handles 15 RPM (free-tier limit)

### Functional Requirements Met

- [x] **FR-001**: Use Gemini API instead of OpenAI
- [x] **FR-002**: Use gemini-1.5-flash model
- [x] **FR-003**: Load credentials from environment variables
- [x] **FR-004**: Maintain same chat API contract
- [x] **FR-005**: Generate responses grounded in RAG context
- [x] **FR-006**: Handle errors gracefully
- [x] **FR-007**: Remove OpenAI dependencies
- [x] **FR-008**: Update environment with Qdrant credentials

## Dependencies

**Story Completion Order**:
1. User Story 1 (P1): Gemini Migration - ONLY story, must complete

**Task Dependencies**:
- T001-T004 can be done in parallel (different files)
- T005 requires T001 complete (dependencies installed)
- T006 requires T001-T005 complete (all changes + install)

## Parallel Execution Examples

**Parallel Group 1** (T001-T004):
- T001: Update requirements.txt
- T002: Update .env
- T003: Update config.py
- T004: Update llm.py

These can all be done simultaneously as they modify different files.

**Sequential**:
- T005: Must wait for T001 (requirements.txt updated)
- T006: Must wait for T001-T005 (all changes complete)

## Implementation Strategy

**MVP Scope**: User Story 1 (only story)

**Incremental Delivery**:
- Milestone 1: File updates (T001-T004) - 15 minutes
- Milestone 2: Install and test (T005-T006) - 10 minutes
- Total: <30 minutes implementation time

## Security Notes

- Real credentials are in `.env` file
- Ensure `.env` is in `.gitignore`
- Never commit credentials to version control
- Use environment variables for production deployment

## Cost Savings

- **Before**: OpenAI GPT-3.5-turbo ~$2-5 per 1000 queries
- **After**: Gemini 1.5 Flash $0 per 1000 queries
- **Savings**: 100% cost reduction
